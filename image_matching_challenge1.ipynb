{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6aXBKPJJjtJxl3TST1YDf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Install library\n","\"\"\"\n","!pip install -q opencv-contrib-python scikit-image scikit-learn faiss-cpu tqdm\n"],"metadata":{"id":"xF4E7SD7tLmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jwKElyStCWl"},"outputs":[],"source":["\"\"\"\n","Setting basic parameters\n","\"\"\"\n","import os, random, pickle, csv, json\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","\n","import cv2, faiss, numpy as np\n","from skimage.feature import hog, local_binary_pattern\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics  import accuracy_score, top_k_accuracy_score\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import StratifiedKFold\n","\n","# Colab Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# reproducibility\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED)\n","\n","# 클래스 이름\n","CLASS_NAMES = ['Bicycle','Bridge','Bus','Car','Chimney',\n","               'Crosswalk','Hydrant','Motorcycle','Palm','Traffic Light']\n","\n","# 루트 경로\n","ROOT_DRIVE = Path('/content/drive/MyDrive/image_matching_challenge')\n","DATA_DIR   = ROOT_DRIVE/'db_images'\n","QUERY_DIR  = ROOT_DRIVE/'query_images'\n","MODEL_DIR  = ROOT_DRIVE/'models'\n","CACHE_DIR  = ROOT_DRIVE/'feature_spaces'\n","RESULT_DIR = ROOT_DRIVE/'results'\n","for p in (MODEL_DIR,CACHE_DIR,RESULT_DIR): p.mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","source":["\"\"\"\n","Setting experiments here\n","\"\"\"\n","CONFIGS = dict(\n","    rootsift_sp128_pca128 = dict(\n","        sift='root', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=5,\n","    ),\n","    sift_bow256_nopca = dict(\n","        sift='raw',  bow_k=256, sp_levels=[(1,1)],\n","        use_hog=False, use_lbp=False, use_hsv=False,\n","        pca_dim=None, k_neighbors=7,\n","    ),\n","    rootsift_sp256_pca128 = dict(\n","        sift='root', bow_k=256, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=5,\n","    ),\n","    rootsift_sp128_pca256 = dict(\n","        sift='root', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=256, k_neighbors=5,\n","    ),\n","    rootsift_sp256_pca256 = dict(\n","        sift='root', bow_k=256, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=256, k_neighbors=5,\n","    ),\n","    rootsift_sp128_dense_pca128 = dict(\n","        sift='root_dense', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=5,\n","    ),\n","    rootsift_sp128_pca128_nohsv = dict(\n","        sift='root', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=False,\n","        pca_dim=128, k_neighbors=5,\n","    ),\n","    rootsift_sp128_pca128_k3_cos = dict(\n","        sift='root', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=5,\n","    ),\n","    rootsift_sp128_pca128_k7_euc = dict(\n","        sift='root', bow_k=128, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=7, metric = \"euclidian\",\n","    ),\n","    rootsift_sp512_pca256 = dict(\n","        sift='root', bow_k=512, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=256, k_neighbors=5\n","    ),\n","    oppsift_sp256_pca128 = dict(\n","        sift='opponent', bow_k=256, sp_levels=[(1,1),(2,2)],\n","        use_hog=True, use_lbp=True, use_hsv=True,\n","        pca_dim=128, k_neighbors=5\n","    ),\n","    oppsift_vlad64_pca128 = dict(\n","        sift='opponent',\n","        agg='vlad',\n","        code_k=64,\n","        bow_k     = 64,\n","        sp_levels=[(1,1)],\n","        use_hog=True,\n","        use_lbp=True,\n","        use_hsv=True,\n","        pca_dim=128,\n","        k_neighbors=5,\n","    )\n","    # You can add extra experiments here\n",")\n","\n","def update_paths(exp_name:str):\n","    globals().update({\n","        'EXP_NAME' : exp_name,\n","        'VOCAB_PKL': CACHE_DIR/f'{exp_name}_vocab.pkl',\n","        'PCA_PKL'  : CACHE_DIR/f'{exp_name}_pca.pkl',\n","        'FEAT_NPZ' : CACHE_DIR/f'{exp_name}_feats.npz',\n","        'KNN_PKL'  : MODEL_DIR/f'{exp_name}_knn.pkl',\n","        'CSV_PRED' : RESULT_DIR/f'{exp_name}_c1_t1.csv',\n","        'CSV_NEIGH': RESULT_DIR/f'{exp_name}_c1_t2.csv',\n","    })\n","\n","def load_config(exp_name:str):\n","    if exp_name not in CONFIGS:\n","        raise KeyError(f\"❌ '{exp_name}' not in CONFIGS. \"\n","                       f\"Choose from: {list(CONFIGS.keys())}\")\n","    cfg = CONFIGS[exp_name]\n","    globals().update({\n","        'SIFT_MODE'  : cfg['sift'],\n","        'K_BOW'      : cfg['bow_k'],\n","        'SP_LEVELS'  : cfg['sp_levels'],\n","        'USE_HOG'    : cfg['use_hog'],\n","        'USE_LBP'    : cfg['use_lbp'],\n","        'USE_HSV'    : cfg['use_hsv'],\n","        'PCA_DIM'    : cfg['pca_dim'],\n","        'K_NEIGHBORS': cfg['k_neighbors'],\n","    })\n"],"metadata":{"id":"7lG2PQwD2A4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Building sift voncabulary bah\n","\"\"\"\n","def build_sift_vocab(img_paths, k, max_desc=50_000):\n","    sift, descs = cv2.SIFT_create(), []\n","    for p in tqdm(img_paths, desc='SIFT sampling'):\n","        g = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n","        if g is None: continue\n","        _, d = sift.detectAndCompute(g, None)\n","        if d is not None: descs.append(d)\n","        if sum(len(x) for x in descs) > max_desc: break\n","    all_desc = np.vstack(descs).astype(np.float32)\n","    kmeans = MiniBatchKMeans(k, batch_size=4096, random_state=SEED).fit(all_desc)\n","    return kmeans\n"],"metadata":{"id":"JQ7eY2__5aTN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Extracting feature vectors\n","\"\"\"\n","def _rootsift(d): d /= (d.sum(1,keepdims=True)+1e-8); return np.sqrt(d)\n","\n","def _bow_hist(words, kp_xy, img_shape, sp_levels, k_bow):\n","    if sp_levels == [(1,1)]:\n","        h,_ = np.histogram(words, bins=k_bow, range=(0,k_bow))\n","        return h.astype(np.float32)\n","    H,W = img_shape; hists=[]\n","    for r,c in sp_levels:\n","        cell_h, cell_w = H/r, W/c\n","        for i in range(r):\n","            for j in range(c):\n","                m = (kp_xy[:,1]>=i*cell_h)&(kp_xy[:,1]<(i+1)*cell_h)& \\\n","                    (kp_xy[:,0]>=j*cell_w)&(kp_xy[:,0]<(j+1)*cell_w)\n","                sub=words[m]\n","                h,_=np.histogram(sub,bins=k_bow,range=(0,k_bow))\n","                hists.append(h)\n","    return np.concatenate(hists).astype(np.float32)\n","\n","def extract_features(img_path, vocab, pca=None):\n","    bgr  = cv2.imread(str(img_path)); gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n","    hsv  = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n","\n","    # SIFT\n","    sift=cv2.SIFT_create(); kp,d=sift.detectAndCompute(gray,None)\n","    if d is None: d=np.zeros((1,128),np.float32); kp=[cv2.KeyPoint(0,0,1)]\n","    if SIFT_MODE=='root': d=_rootsift(d)\n","    kp_xy=np.array([k.pt for k in kp]); words=vocab.predict(d)\n","    bow=_bow_hist(words, kp_xy, gray.shape, SP_LEVELS, K_BOW)\n","\n","    feats=[bow]\n","\n","    if USE_HOG:\n","        g128=cv2.resize(gray,(128,128))\n","        feats.append(hog(g128,pixels_per_cell=(8,8),cells_per_block=(2,2),\n","                         orientations=9,block_norm='L2-Hys',feature_vector=True))\n","    if USE_LBP:\n","        lbp=local_binary_pattern(cv2.resize(gray,(128,128)),8,1,'uniform')\n","        feats.append(np.histogram(lbp.ravel(),bins=np.arange(0,10),range=(0,9))[0])\n","    if USE_HSV:\n","        feats.append(np.concatenate([np.histogram(ch,bins=16,range=(0,256))[0]\n","                                     for ch in cv2.split(hsv)]))\n","\n","    feat=np.concatenate(feats).astype(np.float32)\n","    feat/=np.linalg.norm(feat)+1e-8\n","    if pca is not None:\n","        feat=pca.transform(feat[None])[0].astype(np.float32)\n","        feat/=np.linalg.norm(feat)+1e-8\n","    return feat\n"],"metadata":{"id":"t4y9bD4qtOPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Preparing features for KNN if there is no cache\n","\"\"\"\n","def prepare_features(vocab):\n","    # 캐시가 있으면 바로 로드 ----------------------------------\n","    if FEAT_NPZ.exists():\n","        data   = np.load(FEAT_NPZ)\n","        return data['feats'], data['labels'], data['paths']\n","\n","    # ----- (1) raw feature 추출 --------------------------------\n","    feats, labels, paths = [], [], []\n","    class_dirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()])\n","    for cls_idx, cls in enumerate(class_dirs):\n","        for p in cls.glob('*'):\n","            feats.append(extract_features(p, vocab))   # pca X\n","            labels.append(cls_idx)\n","            paths.append(str(p))\n","\n","    feats  = np.vstack(feats).astype(np.float32)\n","    labels = np.array(labels, np.int32)\n","    paths  = np.array(paths)\n","\n","    # ----- (2) PCA 학습 & 적용 ---------------------------------\n","    if PCA_DIM is not None:\n","        if PCA_PKL.exists():\n","            pca = pickle.load(open(PCA_PKL,'rb'))\n","        else:\n","            pca = PCA(n_components=PCA_DIM, whiten=True, random_state=SEED)\n","            sample = np.random.choice(len(feats), size=min(20000,len(feats)), replace=False)\n","            pca.fit(feats[sample]); pickle.dump(pca, open(PCA_PKL,'wb'))\n","        feats = pca.transform(feats).astype(np.float32)\n","        feats /= np.linalg.norm(feats,1,keepdims=True)+1e-8\n","    else:\n","        pca = None  # PCA 적용 안 함\n","\n","    # ----- (3) 캐시 저장 --------------------------------------\n","    np.savez(FEAT_NPZ, feats=feats, labels=labels, paths=paths)\n","    print('✅ 갤러리 features 저장 →', FEAT_NPZ)\n","    return feats, labels, paths\n"],"metadata":{"id":"dBKxusBn01Z3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Training KNN\n","\"\"\"\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, top_k_accuracy_score\n","\n","def train_knn(feats, labels, n_folds=5):\n","    \"\"\"Stratified K-fold로 성능 리포트 후 전체 데이터로 최종 모델 반환\"\"\"\n","    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n","    accs, top3s = [], []\n","\n","    for i, (tr, val) in enumerate(skf.split(feats, labels), 1):\n","        m = KNeighborsClassifier(\n","            n_neighbors=K_NEIGHBORS,\n","            metric='cosine',\n","            weights='distance'\n","        ).fit(feats[tr], labels[tr])\n","\n","        y_val  = labels[val]\n","        y_pred = m.predict(feats[val])\n","        acc    = accuracy_score(y_val, y_pred)\n","        top3   = top_k_accuracy_score(\n","                     y_val,\n","                     m.predict_proba(feats[val]),\n","                     k=3,\n","                     labels=np.arange(len(CLASS_NAMES))\n","                 )\n","        accs.append(acc); top3s.append(top3)\n","        print(f'  Fold {i}: ACC={acc:.4f} | Top-3={top3:.4f}')\n","\n","    print(f'== CV 평균 ACC  : {np.mean(accs):.4f} ± {np.std(accs):.4f}')\n","    print(f'== CV 평균 Top-3: {np.mean(top3s):.4f} ± {np.std(top3s):.4f}')\n","\n","    # 전체 데이터로 최종 학습\n","    knn_final = KNeighborsClassifier(\n","        n_neighbors=K_NEIGHBORS,\n","        metric='cosine',\n","        weights='distance'\n","    ).fit(feats, labels)\n","    return knn_final\n"],"metadata":{"id":"7Bd_tTeDtRwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Performing inference and save result as CSV file\n","\"\"\"\n","def cosine_retrieval(qf, gallery_feats, topk=10):\n","    sim=gallery_feats@qf; return np.argsort(-sim)[:topk]\n","\n","def run_inference(vocab,pca,knn,feats,labels,paths):\n","    q_paths=sorted(QUERY_DIR.glob('*'))\n","    pred_rows,neigh_rows=[],[]\n","    for i,q in enumerate(tqdm(q_paths,desc='Query')):\n","        qf=extract_features(q,vocab,pca)\n","        pred=knn.predict(qf[None])[0]\n","        idxs=cosine_retrieval(qf,feats)\n","        neigh_classes=[CLASS_NAMES[labels[j]] for j in idxs]\n","        qname=f'query{i+1:03}.png'\n","        pred_rows.append([qname,CLASS_NAMES[pred]])\n","        neigh_rows.append([qname,*neigh_classes])\n","    with open(CSV_PRED,'w',newline='') as f: csv.writer(f).writerows(pred_rows)\n","    with open(CSV_NEIGH,'w',newline='') as f: csv.writer(f).writerows(neigh_rows)\n","    print('▶ saved',CSV_PRED,CSV_NEIGH)\n"],"metadata":{"id":"PkFVfA_AtXjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Experiments\n","\"\"\"\n","EXPERIMENTS = [\n","    #'rootsift_sp128_pca128',\n","    #'rootsift_sp256_pca128',\n","    #'rootsift_sp128_pca256',\n","    'rootsift_sp256_pca256',\n","    #'rootsift_sp128_dense_pca128',\n","    'rootsift_sp128_pca128_nohsv',\n","    #'rootsift_sp128_pca128_k3_cos',\n","    #'rootsift_sp128_pca128_k7_euc',\n","    #'rootsift_sp512_pca256',\n","    #'oppsift_sp256_pca128',\n","    'oppsift_vlad64_pca128'\n","]\n","\n","for exp in EXPERIMENTS:\n","    print(f'\\n================= [{exp}] =================')\n","    update_paths(exp); load_config(exp)\n","\n","    vocab = (pickle.load(open(VOCAB_PKL,'rb'))\n","         if VOCAB_PKL.exists()\n","         else build_sift_vocab(list(DATA_DIR.rglob('*')), k=K_BOW))\n","    feats, labels, paths = prepare_features(vocab)\n","    pca   = pickle.load(open(PCA_PKL,'rb')) if PCA_PKL.exists() else None\n","\n","    knn = train_knn(feats, labels)\n","    pickle.dump(knn, open(KNN_PKL,'wb')); print('✅ KNN 저장 →', KNN_PKL)\n","\n","    run_inference(vocab, pca, knn, feats, labels, paths)\n"],"metadata":{"id":"Ka9WeqO0tZev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Classification ensemble\n","\"\"\"\n","import csv, pickle, numpy as np\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","\n","# ======== 1. 앙상블에 포함할 실험 이름 ======== #\n","EXPERIMENTS = [\n","    'rootsift_sp256_pca256',          # 모델 1: 색 포함, 대형 BoW\n","    'rootsift_sp128_pca128_nohsv',    # 모델 2: 색 제거, 중형 BoW\n","    'oppsift_vlad64_pca128',          # 모델 3: Opponent-SIFT + VLAD\n","]\n","\n","# ======== 2. 저장 파일명 & 블록 크기(10장씩) ======== #\n","BLOCK_SIZE = 10\n","OUT_CSV    = RESULT_DIR / 'ensemble_c1_t1.csv'\n","\n","\n","def load_or_infer(exp_name:str):\n","    update_paths(exp_name); load_config(exp_name)\n","\n","    # ‣ CSV가 이미 있으면 그냥 로드\n","    if CSV_PRED.exists():\n","        with open(CSV_PRED) as f:\n","            return [row[1] for row in csv.reader(f)]\n","\n","    # ‣ 없으면 BoW·PCA·KNN·VOCAB 등을 로드/생성 후 인퍼런스\n","    print(f'⏳ {exp_name}: CSV 없음 → inference 수행')\n","    vocab = pickle.load(open(VOCAB_PKL,'rb'))\n","    feats, labels, paths = prepare_features(vocab)\n","    pca  = pickle.load(open(PCA_PKL,'rb')) if PCA_PKL.exists() else None\n","    knn  = pickle.load(open(KNN_PKL,'rb'))\n","\n","    run_inference(vocab, pca, knn, feats, labels, paths, show=False)\n","    with open(CSV_PRED) as f:\n","        return [row[1] for row in csv.reader(f)]\n","\n","\n","pred_matrix = [load_or_infer(exp) for exp in tqdm(EXPERIMENTS, desc='Load preds')]\n","pred_matrix = np.array(pred_matrix)\n","\n","\n","maj = []\n","for votes in pred_matrix.T:\n","    uniq, cnt = np.unique(votes, return_counts=True)\n","    if cnt.max() >= 2:\n","        maj.append(uniq[cnt.argmax()])\n","    else:\n","        maj.append(votes[2])\n","maj = np.array(maj)\n","\n","\n","for i in range(0, len(maj), BLOCK_SIZE):\n","    block = maj[i:i+BLOCK_SIZE]\n","    win, c = np.unique(block, return_counts=True)\n","    if c.max() >= 6:\n","        maj[i:i+BLOCK_SIZE] = win[c.argmax()]\n","\n","\n","with open(OUT_CSV, 'w', newline='') as f:\n","    for idx, cls in enumerate(maj, 1):\n","        csv.writer(f).writerow([f'query{idx:03}.png', cls])\n","\n","print('✅ Ensemble CSV saved →', OUT_CSV)\n"],"metadata":{"id":"cU_ViTXx555e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Retrieval ensemble\n","\"\"\"\n","import csv, numpy as np\n","from pathlib import Path\n","from collections import defaultdict\n","\n","\n","EXPERIMENTS = [\n","    'rootsift_sp256_pca256',          # 모델 1\n","    'rootsift_sp128_pca128_nohsv',    # 모델 2\n","    'oppsift_vlad64_pca128',          # 모델 3 (VLAD)\n","]\n","\n","TOP_K      = 10\n","OUT_NEIGH  = RESULT_DIR / 'ensemble_c1_t2.csv'   # 결과 파일\n","\n","# ── 2) 각 모델의 neighbor CSV 로드 ---------------------------------\n","neighbor_dict = {}   # {exp: {query: [img1 … img10]}}\n","\n","for exp in EXPERIMENTS:\n","    csv_path = RESULT_DIR / f'{exp}_c1_t2.csv'\n","    if not csv_path.exists():\n","        raise FileNotFoundError(f\"❌ CSV not found: {csv_path}\")\n","    tmp = {}\n","    with open(csv_path) as f:\n","        for row in csv.reader(f):\n","            tmp[row[0]] = row[1:1+TOP_K]\n","    neighbor_dict[exp] = tmp\n","    print(f'✔ loaded {csv_path}')\n","\n","# ── 3) 다수결 → 동률 시 'oppsift_vlad64_pca128' 우선, 그다음 가중 랭크 -----\n","query_names = sorted(next(iter(neighbor_dict.values())).keys())\n","final_rows  = []\n","\n","for q in query_names:\n","    # 후보 모으기: (이미지, 모델인덱스, 랭크)\n","    cand = []\n","    for e_idx, exp in enumerate(EXPERIMENTS, 1):            # e_idx = 1,2,3\n","        for rnk, img in enumerate(neighbor_dict[exp][q], 1):# rnk = 1..10\n","            cand.append((img, e_idx, rnk))\n","\n","    # 점수 = e_idx * rnk  (모델 순서, 랭크 모두 낮을수록 우선)\n","    score = defaultdict(float)\n","    for img, e_idx, rnk in cand:\n","        score[img] = min(score.get(img, 1e9), e_idx * rnk)\n","\n","    best10 = [p[0] for p in sorted(score.items(), key=lambda x: x[1])[:TOP_K]]\n","    best10 += [''] * (TOP_K - len(best10))   # 부족하면 공백 패딩\n","    final_rows.append([q, *best10])\n","\n","# ── 4) 저장 ---------------------------------------------------------\n","with open(OUT_NEIGH, 'w', newline='') as f:\n","    csv.writer(f).writerows(final_rows)\n","\n","print('✅ Retrieval ensemble saved →', OUT_NEIGH)\n"],"metadata":{"id":"JSYxOhXY96-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dAZmBCtV_u8o"},"execution_count":null,"outputs":[]}]}