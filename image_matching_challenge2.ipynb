{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyN0b+VHi9IQyWvIzcu0Csf+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"po2DFMBJWcwu"},"outputs":[],"source":["!pip install torchsummary timm scikit-learn faiss-cpu -q"]},{"cell_type":"code","source":["import warnings, os, copy, time, json, pickle, numpy as np, torch, torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms, models\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics  import accuracy_score, classification_report, top_k_accuracy_score\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","import timm\n","import torch.nn.functional as F\n","from torch.utils.data import WeightedRandomSampler\n","import faiss\n","\n","SEED          = 42\n","torch.manual_seed(SEED); np.random.seed(SEED)"],"metadata":{"id":"m_kIGrP1Widf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!git clone https://github.com/hbcbh1999/recaptcha-dataset.git"],"metadata":{"id":"JBQUY6svWleX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!rm -rf ./recaptcha-dataset/Large/Mountain/\n","#!rm -rf ./recaptcha-dataset/Large/Other/\n","#!rm -rf ./recaptcha-dataset/Large/readme.txt"],"metadata":{"id":"HvzaDzuPWnyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"V8ZK9qX_9WWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CLASS_NAMES   = ['Bicycle','Bridge','Bus','Car','Chimney',\n","                 'Crosswalk','Hydrant','Motorcycle','Palm','Traffic Light']\n","ROOT_DRIVE = Path('/content/drive/MyDrive/image_matching_challenge')\n","DATA_DIR      = ROOT_DRIVE / 'db_images'\n","MODEL_DIR  = ROOT_DRIVE / 'models'\n","FEAT_DIR   = ROOT_DRIVE / 'feature_spaces'\n","MODEL_DIR.mkdir(parents=True, exist_ok=True)\n","FEAT_DIR.mkdir(parents=True, exist_ok=True)\n","NUM_CLASSES   = len(CLASS_NAMES)\n","IMG_SIZE      = 224\n","BATCH_SIZE    = 16            # Convnext: 32 ResNet: 16\n","NUM_EPOCHS    = 20\n","LR            = 3e-4\n","MODEL_NAME    = 'resnet152'    # 'convnext_tiny' | 'resnet152' | 'efficientnet_b3' | 'mobilenet_v3_large' | 'densenet201'\n","WEIGHTS_OUT   = MODEL_DIR / f'{MODEL_NAME}_best.pth'\n","FEAT_CACHE    = FEAT_DIR / f'{MODEL_NAME}_features.npz'\n","KNN_PKL       = MODEL_DIR / f'{MODEL_NAME}_knn.pkl'\n","\n","\n","# 플래그: True → 새로 학습 / False → 저장본 로딩\n","TRAIN_CNN     = not WEIGHTS_OUT.exists()\n","EXTRACT_FEATS = not FEAT_CACHE.exists()\n","FIT_KNN       = not KNN_PKL.exists()\n","\n","n_cls = 10\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('device =', device)\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"X8u_Phm7W5Kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Performing data augmentation and setting dataloader\n","\"\"\"\n","train_tfms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","val_tfms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","full_ds = datasets.ImageFolder(DATA_DIR, transform=train_tfms)\n","indices = np.random.permutation(len(full_ds))\n","split   = int(0.8*len(full_ds))\n","train_idx, val_idx = indices[:split], indices[split:]\n","\n","train_ds = Subset(full_ds, train_idx)\n","val_ds   = Subset(copy.deepcopy(full_ds), val_idx)\n","val_ds.dataset.transform = val_tfms\n","\n","targets_arr  = np.array([t for _, t in full_ds.samples])\n","class_counts = np.bincount(targets_arr, minlength=len(CLASS_NAMES))\n","weights      = 1. / class_counts[targets_arr[train_idx]]\n","weights = torch.as_tensor(weights, dtype=torch.double)\n","weights      = torch.DoubleTensor(weights)\n","train_sampler= WeightedRandomSampler(weights, len(weights), replacement=True)\n","\n","dataloaders = {\n","    'train': DataLoader(train_ds, batch_size=BATCH_SIZE,\n","                        sampler=train_sampler, num_workers=2, pin_memory=True, persistent_workers=False),\n","    'val'  : DataLoader(val_ds,   batch_size=BATCH_SIZE,\n","                        shuffle=False, num_workers=2, pin_memory=True, persistent_workers=False)\n","}\n","print(f\"train / val = {len(train_ds)} / {len(val_ds)}\")\n","\n","\n","class BalancedSoftmaxCE(nn.Module):\n","    def __init__(self, cls_cnt):\n","        super().__init__()\n","        freq = torch.tensor(cls_cnt, dtype=torch.float32)\n","        self.log_freq = (freq / freq.sum()).log()\n","\n","    def forward(self, logits, target):\n","        return F.cross_entropy(logits + self.log_freq.to(logits.device), target)\n","\n","class ArcFaceLoss(nn.Module):\n","    def __init__(self, in_feat, n_cls, s=30.0, m=0.50):\n","        super().__init__()\n","        self.W = nn.Parameter(torch.randn(n_cls, in_feat))\n","        nn.init.xavier_uniform_(self.W); self.s, self.m = s, m\n","\n","    def forward(self, emb, label):\n","        emb = F.normalize(emb); W = F.normalize(self.W)\n","        cos = F.linear(emb, W).clamp(-1+1e-7, 1-1e-7)\n","        theta = torch.acos(cos); target_cos = torch.cos(theta + self.m)\n","        onehot = torch.zeros_like(cos); onehot.scatter_(1, label.view(-1,1), 1.)\n","        logits = cos*(1-onehot) + target_cos*onehot\n","        return F.cross_entropy(self.s*logits, label)"],"metadata":{"id":"9zQmiJETWqDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Building Model\n","\"\"\"\n","\n","def logits_from_emb(model, emb):\n","    if hasattr(model, 'get_classifier'):          # timm ConvNeXt·ViT …\n","        return model.get_classifier()(emb)\n","    elif hasattr(model, 'fc'):                    # torchvision ResNet\n","        return model.fc(emb)\n","    elif hasattr(model, 'classifier'):            # EfficientNet·DenseNet 등\n","        return model.classifier(emb)\n","    else:\n","        raise RuntimeError(\"No classifier head found!\")\n","\n","def build_model(name:str, n_cls:int):\n","    if name.startswith('convnext'):\n","        model = timm.create_model(name, pretrained=True,\n","                                  num_classes=n_cls, global_pool='avg')\n","        in_feat = model.num_features\n","        feat_forward = lambda x: torch.flatten(\n","            model.head.global_pool(model.forward_features(x)), 1)\n","\n","    elif name == 'resnet152':\n","        model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n","        in_feat = model.fc.in_features; model.fc = nn.Linear(in_feat, n_cls)\n","        feat_forward = lambda x: torch.flatten(model.avgpool(\n","            model.layer4(model.layer3(model.layer2(model.layer1(\n","            model.relu(model.bn1(model.conv1(x)))))))),1)\n","\n","    elif name.startswith('efficientnet'):\n","        model = models.__dict__[name](weights='IMAGENET1K_V1')\n","        # 마지막 Linear 교체\n","        in_feat = model.classifier[-1].in_features\n","        model.classifier[-1] = nn.Linear(in_feat, n_cls)\n","        # feature 추출 함수\n","        feat_forward = lambda x: torch.flatten(\n","            model.avgpool(model.features(x)), 1)\n","    else: raise ValueError(\"지원되지 않는 모델\")\n","    return model.to(device), feat_forward, in_feat\n","\n","model, feat_forward, EMB_DIM = build_model(MODEL_NAME, len(CLASS_NAMES))\n","print(f\"{MODEL_NAME}: {sum(p.numel() for p in model.parameters())/1e6:.2f}M params, feat_dim={EMB_DIM}\")\n","\n","# -----------------------------------------------------------\n","# 2-1. 학습 루프\n","# -----------------------------------------------------------\n","if TRAIN_CNN:\n","    print(\">>> training CNN with BalancedSoftmax + ArcFace\")\n","    crit_bs = BalancedSoftmaxCE(class_counts).to(device)\n","    arcloss = ArcFaceLoss(EMB_DIM, len(CLASS_NAMES)).to(device)\n","    optimizer = optim.AdamW(list(model.parameters())+list(arcloss.parameters()),\n","                            lr=LR, weight_decay=1e-2)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n","\n","    best_acc, best_wts = 0., copy.deepcopy(model.state_dict())\n","    for epoch in range(NUM_EPOCHS):\n","        print(f\"\\n[Epoch {epoch+1}/{NUM_EPOCHS}]\")\n","        for phase in ('train','val'):\n","            model.train() if phase=='train' else model.eval()\n","            tot_loss = tot_hit = 0\n","            for X, y in tqdm(dataloaders[phase], leave=False):\n","                X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","                optimizer.zero_grad(set_to_none=True)\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    emb = feat_forward(X)                  # ① 백본 1회\n","                    if emb.dim() == 4:                     # ② (B,C,H,W)→(B,C)\n","                        emb = emb.mean(dim=[2, 3])\n","                    logits = logits_from_emb(model, emb)   # ③ FC만 호출\n","                    loss   = crit_bs(logits, y) + 0.5 * arcloss(emb, y)\n","                    if phase == 'train':\n","                        loss.backward(); optimizer.step()\n","                tot_loss += loss.item()*len(X)\n","                tot_hit  += (logits.argmax(1)==y).sum().item()\n","\n","            epoch_loss = tot_loss/len(dataloaders[phase].dataset)\n","            epoch_acc  = tot_hit / len(dataloaders[phase].dataset)\n","            print(f\" {phase:5s} | loss {epoch_loss:.4f} | acc {epoch_acc:.4f}\")\n","            if phase=='val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_wts = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n","        scheduler.step()\n","\n","    print(f\"\\nBest val acc = {best_acc:.4f}\")\n","    torch.save(best_wts, WEIGHTS_OUT)\n","else:\n","    print(\">>> Found weights – loading\")\n","    model.load_state_dict(torch.load(WEIGHTS_OUT, map_location=device))\n"],"metadata":{"id":"B3anH1taXpBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Caching feature vector\n","\"\"\"\n","def extract_feats(dataset, batch=BATCH_SIZE):\n","    loader = DataLoader(dataset, batch_size=batch, shuffle=False,\n","                        num_workers=0, pin_memory=True)\n","    feats, labels = [], []\n","    model.eval()\n","    with torch.inference_mode():\n","        for X,y in tqdm(loader, desc=\"extract\"):\n","            X = X.to(device)\n","            f = feat_forward(X)              # (B,C,H,W) 또는 (B,C)\n","            if f.dim() == 4:                 # ▲ FIX: 공간 평균\n","                f = f.mean(dim=[2,3])        # (B,C)\n","            f = F.normalize(f, p=2, dim=1)   # ▲ FIX: 이제 L2\n","            feats.append(f.cpu()); labels.append(y)\n","    paths = [dataset.dataset.samples[i][0] for i in dataset.indices] \\\n","            if isinstance(dataset, Subset) \\\n","            else [p for p,_ in dataset.samples]\n","    return torch.cat(feats).numpy(), torch.cat(labels).numpy(), np.array(paths)\n","\n","if EXTRACT_FEATS:\n","    full_ds_eval           = copy.deepcopy(full_ds)\n","    full_ds_eval.transform = val_tfms\n","    feats, labels, paths   = extract_feats(full_ds_eval)\n","    np.savez_compressed(FEAT_CACHE, feats=feats, labels=labels, paths=paths)\n","else:\n","    data   = np.load(FEAT_CACHE, allow_pickle=True)\n","    feats, labels, paths = data['feats'], data['labels'], data['paths']"],"metadata":{"id":"sEb19Y03X7hL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_feats, val_feats = feats[train_idx], feats[val_idx]\n","train_lbls , val_lbls  = labels[train_idx], labels[val_idx]\n","\n","gallery_feats = feats\n","gallery_lbls  = labels\n","\n","if FIT_KNN:\n","    knn = KNeighborsClassifier(n_neighbors=5, metric='cosine', weights='distance')\n","    knn.fit(gallery_feats, gallery_lbls)         # ★MOD\n","    with open(KNN_PKL, 'wb') as f: pickle.dump(knn, f)\n","else:\n","    with open(KNN_PKL, 'rb') as f: knn = pickle.load(f)\n","\n","val_pred = knn.predict(val_feats)\n","print(\"\\nKNN val ACC =\", accuracy_score(val_lbls, val_pred))\n","print(classification_report(val_lbls, val_pred, target_names=CLASS_NAMES, digits=3))\n","\n","# ----- (선택) Faiss 인덱스 – 대규모용 ----------\n","# faiss_index = faiss.IndexFlatIP(train_feats.shape[1])\n","# faiss_index.add(train_feats.astype('float32'))\n","\n","def k_reciprocal_rerank(qf, gf, initial_rank, k1=20, k2=6, lamb=0.3):\n","    cos   = (gf @ qf.T).squeeze(); dist = 1 - cos\n","    V = np.zeros_like(dist)\n","    for i in initial_rank[:k1]:\n","        sim_i = gf @ gf[i]; ind_i = sim_i.argsort()[-k2:]\n","        V[i] = sim_i[ind_i].mean()\n","    V = (V - V.min()) / (V.max() - V.min() + 1e-12)\n","    return ((1-lamb)*dist + lamb*(1-V)).argsort()\n","\n","def retrieve(query_idx: int, k: int = 10, k1: int = 20, k2: int = 6, lamb: float = 0.3):\n","    \"\"\"\n","    • query_idx : val 세트에서 질의로 쓸 인덱스\n","    • k         : 최종으로 돌려줄 개수\n","    \"\"\"\n","    # 1) 쿼리 · 갤러리 feature\n","    qf = val_feats[query_idx]                       # shape (D,)\n","    gf = gallery_feats                             # shape (N, D)\n","\n","    # 2) cosine 거리 (= 1 – cosine similarity) 전체 계산\n","    cos_all  = gf @ qf               # similarity  (N,)\n","    dist_all = 1.0 - cos_all         # distance    (N,)\n","\n","    # 3) 초기 랭크 & k-reciprocal re-rank\n","    init_rank = dist_all.argsort()[:50]            # 50 개 정도면 충분\n","    rank      = k_reciprocal_rerank(qf, gf, init_rank,\n","                                    k1=k1, k2=k2, lamb=lamb)\n","\n","    # 4) 최종 상위 k\n","    final_idx = rank[:k]\n","    neigh_info = [\n","        (paths[i],\n","         CLASS_NAMES[gallery_lbls[i]],\n","         float(dist_all[i]))\n","        for i in final_idx\n","    ]\n","    return neigh_info\n","\n","print(\"\\n[Query 예시]\")\n","for r,(p,c,d) in enumerate(retrieve(0,10),1):\n","    print(f\"{r:02d}. {Path(p).name:30s} | {c:12s} | dist={d:.4f}\")"],"metadata":{"id":"bSjxNIWQZV6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Performing classification and retrieval for query images\n","\"\"\"\n","\n","import pickle, numpy as np, torch\n","from PIL import Image\n","from sklearn.preprocessing import normalize\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","import csv\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","\n","# --- 0) Drive 경로 및 파일 존재 여부 -----------------------------------------\n","ROOT_DRIVE = Path('/content/drive/MyDrive/image_matching_challenge')\n","WEIGHTS_OUT = ROOT_DRIVE / 'models' / f'{MODEL_NAME}_best.pth'\n","FEAT_CACHE  = ROOT_DRIVE / 'feature_spaces' / f'{MODEL_NAME}_features.npz'\n","KNN_PKL     = ROOT_DRIVE / 'models' / f'{MODEL_NAME}_knn.pkl'\n","RESULT_DIR_1 = ROOT_DRIVE / 'results' / f'{MODEL_NAME}_c2_t1_a1.csv'\n","RESULT_DIR_2 = ROOT_DRIVE / 'results' / f'{MODEL_NAME}_c2_t2_a2.csv'\n","\n","assert WEIGHTS_OUT.exists(), \"❌ CNN 가중치(pth)가 없습니다.\"\n","assert FEAT_CACHE.exists() , \"❌ feature 캐시(npz)가 없습니다.\"\n","assert KNN_PKL.exists()    , \"❌ KNN 모델(pkl)이 없습니다.\"\n","\n","# --- 1) 모델 & 보조 객체 로드 -------------------------------------------------\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model, feat_forward, _ = build_model(MODEL_NAME, len(CLASS_NAMES))\n","model.load_state_dict(torch.load(WEIGHTS_OUT, map_location=device))\n","model.eval()\n","\n","# 특징 벡터·레이블·경로 로드 (retrieval용)\n","data   = np.load(FEAT_CACHE, allow_pickle=True)\n","feats  = normalize(data['feats'].astype('float32'))      # L2 정규화 이미 했으면 제거\n","labels = data['labels']\n","paths  = data['paths']\n","\n","# KNN 로드\n","with open(KNN_PKL, 'rb') as f:\n","    knn = pickle.load(f)\n","\n","# --- 2) 쿼리 이미지 → 분류 + Retrieval ----------------------------------------\n","def classify_and_retrieve(query_paths, k=10, verbose=True, pred_csv=RESULT_DIR_1, neigh_csv=RESULT_DIR_2, show_img=True):\n","    \"\"\"\n","    Args\n","    ----\n","    query_paths : list[str | Path]\n","        분류·검색할 이미지 경로 리스트\n","    k           : int, optional\n","        Retrieval 상위 k개\n","    verbose     : bool\n","        True면 결과를 print, False면 dict 반환만\n","    \"\"\"\n","    pred_rows, neigh_rows = [], []      # CSV에 쓸 레코드 누적\n","    results = []\n","\n","    for idx_query, qpath in enumerate(tqdm(query_paths, desc=\"query\")):\n","        img = Image.open(qpath).convert('RGB')\n","        if show_img:                                  # ★ 추가\n","            plt.figure(figsize=(3,3))\n","            plt.imshow(img); plt.axis('off')\n","            plt.title(f\"Query {idx_query+1}: {Path(qpath).name}\")\n","            plt.show()\n","\n","\n","        x   = val_tfms(img).unsqueeze(0).to(device)      # (1,3,H,W)\n","        # 1) 분류\n","        with torch.inference_mode():\n","            logits = model(x)\n","            pred   = logits.argmax(1).item()\n","            emb_t  = F.normalize(feat_forward(x), dim=1)   # (1, D)\n","            emb    = emb_t.cpu().numpy().squeeze()\n","\n","        # 2) 갤러리 전체에 대한 cosine → distance\n","        cos_all  = feats @ emb               # (N,)\n","        dist_all = 1.0 - cos_all             # (N,)\n","\n","        # 3) 초기 랭크 50개 → k-reciprocal 재랭크\n","        init_rank = dist_all.argsort()[:50]\n","        rank      = k_reciprocal_rerank(emb, feats, init_rank)\n","        final_idx = rank[:k]\n","\n","        # 4) 이웃 정보\n","        neigh_info = [\n","            (paths[i], CLASS_NAMES[labels[i]], float(dist_all[i]))\n","            for i in final_idx\n","        ]\n","\n","        # 5) CSV용 행\n","        qname_png   = f'query{idx_query+1:03}.png'\n","        pred_rows.append([qname_png, CLASS_NAMES[pred]])\n","\n","        neigh_labels = [CLASS_NAMES[labels[i]] for i in final_idx]\n","        neigh_rows.append([qname_png, *neigh_labels])\n","\n","        if verbose:\n","            print(f\"\\n🖼️  Query: {Path(qpath).name}\")\n","            print(f\"   ➤ Predicted class: {CLASS_NAMES[pred]}\")\n","            print(f\"   ➤ Top-{k} nearest images\")\n","            for r, (p, cls, d) in enumerate(neigh_info, 1):\n","                print(f\"     {r:02d}. {Path(p).name:30s} | {cls:12s} | dist={d:.4f}\")\n","\n","        results.append({\n","            'query'     : str(qpath),\n","            'prediction': CLASS_NAMES[pred],\n","            'neighbors' : neigh_info\n","        })\n","\n","    with open(pred_csv,  'w', newline='') as f1:\n","      csv.writer(f1).writerows(pred_rows)\n","\n","    with open(neigh_csv, 'w', newline='') as f2:\n","      csv.writer(f2).writerows(neigh_rows)\n","\n","    print(f\"\\n✅ Saved {len(pred_rows)} predictions → {pred_csv}\")\n","    print(f\"✅ Saved {len(neigh_rows)} neighbor lists → {neigh_csv}\")\n","\n","    return results\n","\n","# --- 3) 사용 예 ---------------------------------------------------------------\n","# ① 단일 이미지\n","# classify_and_retrieve(['/content/sample_data/cat.png'], k=8)\n","\n","# ② 폴더 안 모든 JPG\n","query_dir = Path('/content/drive/MyDrive/image_matching_challenge/query_images')\n","valid_ext = {'.jpg', '.jpeg', '.png'}\n","query_list = sorted(p for p in query_dir.rglob('*') if p.suffix.lower() in valid_ext)\n","print(f\"↳ found {len(query_list)} query images\")\n","classify_and_retrieve(query_list, k=10, verbose=True)\n"],"metadata":{"id":"bEkHyAXDxUSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Ensemble + Save Results Cell ---\n","from pathlib import Path\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.preprocessing import normalize\n","from PIL import Image\n","import csv\n","\n","# 사전 정의됨: build_model, val_tfms, NUM_CLASSES, CLASS_NAMES\n","\n","# 1) 모델 & 데이터 경로 설정\n","MODEL_DIR   = Path('/content/drive/MyDrive/image_matching_challenge/models')\n","FEAT_DIR    = Path('/content/drive/MyDrive/image_matching_challenge/feature_spaces')\n","QUERY_DIR   = Path('/content/drive/MyDrive/image_matching_challenge/query_images')\n","RESULT_DIR  = Path('/content/drive/MyDrive/image_matching_challenge/results')\n","RESULT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# 2) 앙상블 대상 모델명 및 파일명 프리픽스\n","MODEL_NAMES = ['convnext_tiny', 'resnet152']\n","SAVE_PREFIX = '_'.join(MODEL_NAMES) + '_ensemble'\n","PRED_CSV    = RESULT_DIR / f\"{SAVE_PREFIX}_predictions.csv\"\n","NEIGH_CSV   = RESULT_DIR / f\"{SAVE_PREFIX}_neighbors.csv\"\n","\n","# 3) 디바이스\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 4) 모델 로드\n","models_dict = {}\n","for name in MODEL_NAMES:\n","    m, fwd, _ = build_model(name, NUM_CLASSES)\n","    m.load_state_dict(torch.load(MODEL_DIR/f'{name}_best.pth', map_location=device))\n","    m.to(device).eval()\n","    models_dict[name] = (m, fwd)\n","\n","# 5) 피쳐 스페이스 로드 및 결합\n","conv = np.load(FEAT_DIR/'convnext_tiny_features.npz', allow_pickle=True)\n","res  = np.load(FEAT_DIR/'resnet152_features.npz', allow_pickle=True)\n","feats = np.concatenate([conv['feats'], res['feats']], axis=1).astype('float32')\n","feats = normalize(feats)\n","labels = conv['labels']\n","paths  = conv['paths']\n","\n","# 6) 앙상블 분류+리트리벌 + CSV 저장\n","\n","def ensemble_classify_and_retrieve_and_save(query_paths, k=10):\n","    pred_rows = []\n","    neigh_rows = []\n","\n","    for idx, qpath in enumerate(query_paths, start=1):\n","        img = Image.open(qpath).convert('RGB')\n","        x   = val_tfms(img).unsqueeze(0).to(device)\n","\n","        with torch.inference_mode():\n","            # logits ensemble\n","            logits = torch.stack([m(x) for m,fwd in models_dict.values()]).mean(0)\n","            pred   = logits.argmax(1).item()\n","            # embedding ensemble\n","            embs = [F.normalize(fwd(x).view(x.size(0), -1), dim=1)\n","                    for m,fwd in models_dict.values()]\n","            emb = torch.cat(embs, dim=1).cpu().numpy().squeeze()\n","\n","        # retrieval\n","        dist_all = 1.0 - (feats @ emb)\n","        idxs     = dist_all.argsort()[:k]\n","\n","        # CSV rows 준비\n","        qname = f\"query{idx:03}.png\"\n","        pred_rows.append([qname, CLASS_NAMES[pred]])\n","        neigh_labels = [CLASS_NAMES[labels[i]] for i in idxs]\n","        neigh_rows.append([qname, *neigh_labels])\n","\n","    # 저장\n","    with open(PRED_CSV, 'w', newline='') as f1:\n","        csv.writer(f1).writerows(pred_rows)\n","    with open(NEIGH_CSV, 'w', newline='') as f2:\n","        csv.writer(f2).writerows(neigh_rows)\n","\n","    print(f\"✅ Saved predictions -> {PRED_CSV}\")\n","    print(f\"✅ Saved neighbors   -> {NEIGH_CSV}\")\n","\n","    return pred_rows, neigh_rows\n","\n","# 7) 실행\n","valid_exts = {'.jpg', '.jpeg', '.png'}\n","queries    = sorted(p for p in QUERY_DIR.rglob('*') if p.suffix.lower() in valid_exts)\n","print(f\"↳ found {len(queries)} queries\")\n","ensemble_classify_and_retrieve_and_save(queries, k=10)\n"],"metadata":{"id":"yTD_aqWDQQ3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"1DphJWMGJKxD"}}]}